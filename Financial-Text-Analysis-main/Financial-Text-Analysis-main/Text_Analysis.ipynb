{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the excel file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \n",
    "There are Two classes contains DataExtraction from Web page and Text Analysis in Next Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data from Web page contains title and Content\n",
    "class DataExtract:\n",
    "    def __init__(self,index):\n",
    "        self.index = index\n",
    "    #Extracting text from url with index\n",
    "    def extract_text_from_url(self):\n",
    "        url = xlsx['URL'][self.index]\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content,'html.parser')\n",
    "        Title = soup.find('h1')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text_only = [p.get_text(strip=True) for p in paragraphs]\n",
    "        text = text_only[16:-2]\n",
    "        Raw_text = ''.join(text)\n",
    "        return Title,Raw_text\n",
    "\n",
    "\n",
    "#TextData Analsysis Class\n",
    "class TextAnalysis:\n",
    "    def __init__(self,StopWords,Postive,Negitive,Raw_text):\n",
    "        self.StopWords = StopWords\n",
    "        self.Postive = Postive\n",
    "        self.Negitive = Negitive\n",
    "        self.Raw_text  = Raw_text\n",
    " \n",
    "    #Cleaning Data removing .?'..etc and lowering the text\n",
    "    def Cleaning(self):\n",
    "        clean_text = re.sub(r'\\s+', ' ', self.Raw_text)\n",
    "        clean_text = re.sub(r'\\n+', '\\n', clean_text) \n",
    "        clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text.lower())\n",
    "        return clean_text\n",
    "    \n",
    "    #Removal of Stopwords that present in Stopwords folder.\n",
    "    def StopWords_removal(self):\n",
    "        #StopWords already extracted and in list of StopWords variable\n",
    "        clean_text = self.Cleaning()\n",
    "        Filtered_Words = [word for word in clean_text.split() if word.lower() not in self.StopWords]\n",
    "        \n",
    "        return Filtered_Words\n",
    "    \n",
    "    #Finding Postive and Negitive Words\n",
    "    def Words_Behavior(self,Positive,Negitive):\n",
    "        positive_words = list()\n",
    "        negitive_words = list()\n",
    "        filtered_words = self.StopWords_removal()\n",
    "        for word in filtered_words:\n",
    "            if word in Positive:\n",
    "                positive_words.append(word)\n",
    "            elif word in Negitive:\n",
    "                negitive_words.append(word)    \n",
    "\n",
    "        positive_words = nltk.word_tokenize(' '.join(positive_words))\n",
    "        negitive_words =nltk.word_tokenize(' '.join(negitive_words))\n",
    "        return positive_words,negitive_words\n",
    "    \n",
    "    #Step Two predict Words Score\n",
    "    def Scores(self):\n",
    "        positive_words,negitive_words = self.Words_Behavior(self.Postive,self.Negitive)\n",
    "        filtered_words = self.StopWords_removal()\n",
    "        postive_score = np.ones((1,len(positive_words))).astype('int')\n",
    "        negitive_score  = np.ones((1,len(negitive_words))).astype('int') *-1\n",
    "        positive_df = pd.DataFrame({'positive_words': positive_words, 'Score': postive_score[0]})\n",
    "        negitive_df = pd.DataFrame({'negitive_words': negitive_words, 'Score': negitive_score[0]})\n",
    "        Postive_words_Score = positive_df['Score'].sum()\n",
    "        Negitive_words_Score = negitive_df['Score'].sum()\n",
    "        Polarity_Score = ((positive_df['Score'].sum())-(-negitive_df['Score'].sum())) /((positive_df['Score'].sum()+negitive_df['Score'].sum()) + 0.000001)\n",
    "        Subjective_Score =((positive_df['Score'].sum())+(-(negitive_df['Score'].sum()))) / (len(filtered_words) + 0.000001)\n",
    "        return Postive_words_Score,Negitive_words_Score,Polarity_Score,Subjective_Score\n",
    "    \n",
    "    #text data extraction\n",
    "    def text(self):\n",
    "        Raw_text =''.join(self.Raw_text)\n",
    "        sentences = re.split(r'\\.+\\s*', Raw_text)\n",
    "        words = [word for sentence in sentences for word in sentence.split()]\n",
    "        return sentences, words\n",
    "    \n",
    "    #Syllable Count\n",
    "    def syllable_(self):\n",
    "        sentences,_ = self.text()\n",
    "        def count_syllables(word):\n",
    "\n",
    "            # Remove common suffixes that do not contribute to syllable count\n",
    "            word = re.sub(r'(es|ed|e$)', '', word, flags=re.IGNORECASE)\n",
    "    \n",
    "            # Count vowels (excluding consecutive vowels)\n",
    "            vowels = re.findall(r'[aeiouy]+', word, flags=re.IGNORECASE)\n",
    "    \n",
    "            # Adjust for words starting with 'y' as it can be a consonant or a vowel\n",
    "            if word and word[0].lower() == 'y' and not vowels:\n",
    "                return 1\n",
    "            else:\n",
    "                return len(vowels)\n",
    "\n",
    "        def count_syllables_per_word(text):\n",
    "            # Tokenize the text into words\n",
    "            words = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "            # Count syllables for each word\n",
    "            syllable_counts = [count_syllables(word) for word in words]\n",
    "    \n",
    "            return syllable_counts\n",
    "        text = ''.join(sentences)\n",
    "        syllable_counts = count_syllables_per_word(text)\n",
    "\n",
    "        # Create a dataframe from the word and syllable count lists\n",
    "        data = {'Word': re.findall(r'\\b\\w+\\b', text), 'Syllable Value': np.array(syllable_counts)}\n",
    "        syllables_df = pd.DataFrame(data)\n",
    "        return syllables_df,data\n",
    "    \n",
    "\n",
    "    # Analysis Readability Index\n",
    "    def Analysis_of_Readability(self):\n",
    "        sentences,words = self.text()\n",
    "        syllables_df,_ = self.syllable_()\n",
    "        Average_sentence_length = (len(words))/(len(sentences))\n",
    "        Percentage_of_Complex_Words = ((len(syllables_df[syllables_df['Syllable Value']>2]))/(len(words))) *100\n",
    "        Fog_Index = 0.4 * (Average_sentence_length+Percentage_of_Complex_Words)\n",
    "        return Average_sentence_length,Percentage_of_Complex_Words,Fog_Index\n",
    "    \n",
    "\n",
    "    #Average Number of Sentence\n",
    "    def Avg_sen(self):\n",
    "        sentences,words = self.text()\n",
    "        Average_number_words_per_sentence = (len(words))/(len(sentences))\n",
    "        return Average_number_words_per_sentence\n",
    "    \n",
    "\n",
    "    #Complex Word Count\n",
    "    def Complex_count(self):\n",
    "        Syllables_df,_ = self.syllable_()\n",
    "        Df = Syllables_df[Syllables_df['Syllable Value']>2]\n",
    "        return Df\n",
    "    \n",
    "\n",
    "    #Word Count\n",
    "    def Word_count(self):\n",
    "        #Get filtered words from stopwords removal fun\n",
    "        filtered_words = self.StopWords\n",
    "        return len(filtered_words)\n",
    "    \n",
    "\n",
    "    #Syllable Count per word\n",
    "    def Syllable_Count(self):\n",
    "        _,data= self.syllable_()\n",
    "        return data\n",
    "    \n",
    "\n",
    "    #Personal Pronouns\n",
    "    def Personal_Pronouns(self):\n",
    "        Pronoun_list = [\"I\",\"we\", \"my\", \"ours\",\"us\"]\n",
    "        sentences,_ = self.text()\n",
    "        def personal_pronouns(sentences):\n",
    "            #creating regex \n",
    "            reg = re.compile(r'\\b(?:'+'|'.join(Pronoun_list) + r')\\b')\n",
    "            #matching words\n",
    "            match = re.findall(reg, ''.join(sentences))\n",
    "            #counting all reg\n",
    "            count = len(match)\n",
    "            return count\n",
    "        \n",
    "        return personal_pronouns(sentences)\n",
    "\n",
    "\n",
    "    #Average Word Length\n",
    "    def Wordlen(self):\n",
    "        _,words = self.text()\n",
    "        Wordlength_df = pd.DataFrame({\"words\":words, \"length_of_word\":[len(word) for word in words]})\n",
    "        Avg_len = (Wordlength_df['length_of_word'].sum())/(len(Wordlength_df['words']))\n",
    "        return Avg_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening stopwords folder files with different text baises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_.txt\", 'w') as w:\n",
    "    with open(\"StopWords-20240110T154020Z-001/StopWords/StopWords_Currencies.txt\",'r',encoding='latin-1') as f1:\n",
    "        w.write(f1.read())\n",
    "    with open(\"StopWords-20240110T154020Z-001/StopWords/StopWords_DatesandNumbers.txt\", 'r',) as f2:\n",
    "        w.write(f2.read())\n",
    "    with open(\"StopWords-20240110T154020Z-001/StopWords/StopWords_Geographic.txt\", 'r') as f3:\n",
    "        w.write(f3.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords.txt\", 'w') as w:\n",
    "    with open(\"StopWords-20240110T154020Z-001/StopWords/StopWords_Auditor.txt\",'r') as f1:\n",
    "        w.write(f1.read())\n",
    "    with open(\"StopWords-20240110T154020Z-001/StopWords/StopWords_Generic.txt\", 'r') as f2:\n",
    "        w.write(f2.read())\n",
    "    with open(\"StopWords-20240110T154020Z-001/StopWords/StopWords_GenericLong.txt\", 'r') as f3:\n",
    "        w.write(f3.read())\n",
    "    with open('StopWords-20240110T154020Z-001/StopWords/StopWords_Names.txt', 'r') as f4:\n",
    "        w.write(f4.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('StopWords_.txt','r') as f:\n",
    "    stopwords_spc = ''.join(f.readlines()).lower()\n",
    "stopwords_spc_list = [line.split('|')[0].strip().lower() for line in stopwords_spc.split('\\n') if line]\n",
    "\n",
    "with open('StopWords.txt', 'r') as r:\n",
    "    stop_words_two = [line.strip().lower() for line in r.readlines()]\n",
    "\n",
    "stopwords = stop_words_two + stopwords_spc_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Master Dictionary folder extracting postive and negitive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MasterDictionary-20240110T154020Z-001/MasterDictionary/positive-words.txt','r') as r:\n",
    "    positive = [line.strip().lower() for line in r.readlines()]\n",
    "with open('MasterDictionary-20240110T154020Z-001/MasterDictionary/negative-words.txt','r',encoding='latin-1') as r:\n",
    "    negitive= [line.strip().lower() for line in r.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xlsx['URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running all URLS at a time to perform Text-Analysis and Data Extraction \n",
    "\n",
    "### IT Takes minimum 25 mins(because of Web Crawling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_list = []\n",
    "for i in range(0,len(xlsx['URL'])):\n",
    "        Raw_text = DataExtract(i)\n",
    "        title,Raw_text = Raw_text.extract_text_from_url()\n",
    "        text = TextAnalysis(StopWords=stopwords,Postive=positive,Negitive=negitive,Raw_text=Raw_text)\n",
    "        Postive_Score,Negitive_Score,Polarity_Score,Subjectivity_Score = text.Scores()\n",
    "        Average_sentence_length,Percentage_of_Complex_Words,Fog_Index = text.Analysis_of_Readability()\n",
    "        Average_number_words_per_sentence= text.Avg_sen()\n",
    "        Word_count = text.Word_count()\n",
    "        Syllable_per_word = [text.Syllable_Count()]\n",
    "        Personal_Pronouns = text.Personal_Pronouns()\n",
    "        Avg_word_len = text.Wordlen()\n",
    "        Output_list.append([Postive_Score,Negitive_Score,Polarity_Score,round(Subjectivity_Score,2),round(Average_sentence_length,2),round(Percentage_of_Complex_Words,2),round(Fog_Index,2),round(Average_number_words_per_sentence,2),Word_count,Syllable_per_word,Personal_Pronouns,round(Avg_word_len)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrame using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUN','AVG WORD LENGTH']\n",
    "og = pd.DataFrame(Output_list,columns=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "og.insert(0,'URL_ID', xlsx['URL_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "og.insert(1,'URL',xlsx['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             URL_ID                                                URL  \\\n",
      "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
      "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
      "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
      "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
      "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
      "..              ...                                                ...   \n",
      "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
      "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
      "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
      "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
      "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
      "\n",
      "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
      "0                5              -1             1.0                0.04   \n",
      "1               51             -27             1.0                0.11   \n",
      "2               39             -23             1.0                0.10   \n",
      "3               37             -70             1.0                0.18   \n",
      "4               20              -8             1.0                0.08   \n",
      "..             ...             ...             ...                 ...   \n",
      "95              28             -51             1.0                0.14   \n",
      "96              23             -34             1.0                0.13   \n",
      "97               3               0             1.0                0.03   \n",
      "98              14              -2             1.0                0.06   \n",
      "99              30             -50             1.0                0.18   \n",
      "\n",
      "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
      "0                 13.56                        18.31      12.74   \n",
      "1                 17.49                        21.59      15.64   \n",
      "2                 18.71                        29.40      19.24   \n",
      "3                 20.13                        27.18      18.92   \n",
      "4                 17.32                        20.14      14.98   \n",
      "..                  ...                          ...        ...   \n",
      "95                21.62                        20.42      16.82   \n",
      "96                27.62                        12.49      16.05   \n",
      "97                28.50                        22.81      20.52   \n",
      "98                18.16                        14.56      13.09   \n",
      "99                28.03                        17.73      18.31   \n",
      "\n",
      "    AVG NUMBER OF WORDS PER SENTENCE  WORD COUNT  \\\n",
      "0                              13.56       14104   \n",
      "1                              17.49       14104   \n",
      "2                              18.71       14104   \n",
      "3                              20.13       14104   \n",
      "4                              17.32       14104   \n",
      "..                               ...         ...   \n",
      "95                             21.62       14104   \n",
      "96                             27.62       14104   \n",
      "97                             28.50       14104   \n",
      "98                             18.16       14104   \n",
      "99                             28.03       14104   \n",
      "\n",
      "                                    SYLLABLE PER WORD  PERSONAL PRONOUN  \\\n",
      "0   [{'Word': ['We', 'have', 'seen', 'a', 'huge', ...                 1   \n",
      "1   [{'Word': ['Throughout', 'history', 'from', 't...                 3   \n",
      "2   [{'Word': ['IntroductionIn', 'the', 'span', 'o...                13   \n",
      "3   [{'Word': ['The', 'way', 'we', 'live', 'work',...                 4   \n",
      "4   [{'Word': ['The', 'year', '2040', 'is', 'poise...                 4   \n",
      "..                                                ...               ...   \n",
      "95  [{'Word': ['Epidemics', 'in', 'general', 'have...                 2   \n",
      "96  [{'Word': ['COVID', '19', 'has', 'bought', 'th...                 5   \n",
      "97  [{'Word': ['Handicrafts', 'is', 'an', 'art', '...                 0   \n",
      "98  [{'Word': ['I', 'would', 'rather', 'pay', 'cas...                 2   \n",
      "99  [{'Word': ['As', 'business', 'close', 'to', 'h...                 3   \n",
      "\n",
      "    AVG WORD LENGTH  \n",
      "0                 5  \n",
      "1                 6  \n",
      "2                 6  \n",
      "3                 6  \n",
      "4                 6  \n",
      "..              ...  \n",
      "95                5  \n",
      "96                5  \n",
      "97                6  \n",
      "98                5  \n",
      "99                5  \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "og.to_excel('Text_Analysis.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "og.to_csv('Text_Analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUN</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>13.56</td>\n",
       "      <td>18.31</td>\n",
       "      <td>12.74</td>\n",
       "      <td>13.56</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['We', 'have', 'seen', 'a', 'huge', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>51</td>\n",
       "      <td>-27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>17.49</td>\n",
       "      <td>21.59</td>\n",
       "      <td>15.64</td>\n",
       "      <td>17.49</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['Throughout', 'history', 'from', 't...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>39</td>\n",
       "      <td>-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>18.71</td>\n",
       "      <td>29.40</td>\n",
       "      <td>19.24</td>\n",
       "      <td>18.71</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['IntroductionIn', 'the', 'span', 'o...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>37</td>\n",
       "      <td>-70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>20.13</td>\n",
       "      <td>27.18</td>\n",
       "      <td>18.92</td>\n",
       "      <td>20.13</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['The', 'way', 'we', 'live', 'work',...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>20</td>\n",
       "      <td>-8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>17.32</td>\n",
       "      <td>20.14</td>\n",
       "      <td>14.98</td>\n",
       "      <td>17.32</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['The', 'year', '2040', 'is', 'poise...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blackassign0006</td>\n",
       "      <td>https://insights.blackcoffer.com/the-rise-of-t...</td>\n",
       "      <td>84</td>\n",
       "      <td>-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>20.14</td>\n",
       "      <td>25.15</td>\n",
       "      <td>18.12</td>\n",
       "      <td>20.14</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['Entertainment', 'is', 'giving', 'p...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blackassign0007</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>23</td>\n",
       "      <td>-36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.77</td>\n",
       "      <td>20.51</td>\n",
       "      <td>14.51</td>\n",
       "      <td>15.77</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['Cybercrime', 'is', 'the', 'most', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blackassign0008</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-inter...</td>\n",
       "      <td>30</td>\n",
       "      <td>-9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>15.92</td>\n",
       "      <td>31.79</td>\n",
       "      <td>19.09</td>\n",
       "      <td>15.92</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['Introduction', 'The', 'year', '203...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blackassign0009</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>39</td>\n",
       "      <td>-48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>16.55</td>\n",
       "      <td>31.15</td>\n",
       "      <td>19.08</td>\n",
       "      <td>16.55</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['As', 'technology', 'continues', 't...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blackassign0010</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>57</td>\n",
       "      <td>-66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>19.18</td>\n",
       "      <td>22.80</td>\n",
       "      <td>16.79</td>\n",
       "      <td>19.18</td>\n",
       "      <td>14104</td>\n",
       "      <td>[{'Word': ['Understanding', 'Cybercrime', 'An'...</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "5  blackassign0006  https://insights.blackcoffer.com/the-rise-of-t...   \n",
       "6  blackassign0007  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "7  blackassign0008  https://insights.blackcoffer.com/rise-of-inter...   \n",
       "8  blackassign0009  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "9  blackassign0010  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               5              -1             1.0                0.04   \n",
       "1              51             -27             1.0                0.11   \n",
       "2              39             -23             1.0                0.10   \n",
       "3              37             -70             1.0                0.18   \n",
       "4              20              -8             1.0                0.08   \n",
       "5              84             -25             1.0                0.10   \n",
       "6              23             -36             1.0                0.12   \n",
       "7              30              -9             1.0                0.08   \n",
       "8              39             -48             1.0                0.15   \n",
       "9              57             -66             1.0                0.17   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                13.56                        18.31      12.74   \n",
       "1                17.49                        21.59      15.64   \n",
       "2                18.71                        29.40      19.24   \n",
       "3                20.13                        27.18      18.92   \n",
       "4                17.32                        20.14      14.98   \n",
       "5                20.14                        25.15      18.12   \n",
       "6                15.77                        20.51      14.51   \n",
       "7                15.92                        31.79      19.09   \n",
       "8                16.55                        31.15      19.08   \n",
       "9                19.18                        22.80      16.79   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  WORD COUNT  \\\n",
       "0                             13.56       14104   \n",
       "1                             17.49       14104   \n",
       "2                             18.71       14104   \n",
       "3                             20.13       14104   \n",
       "4                             17.32       14104   \n",
       "5                             20.14       14104   \n",
       "6                             15.77       14104   \n",
       "7                             15.92       14104   \n",
       "8                             16.55       14104   \n",
       "9                             19.18       14104   \n",
       "\n",
       "                                   SYLLABLE PER WORD  PERSONAL PRONOUN  \\\n",
       "0  [{'Word': ['We', 'have', 'seen', 'a', 'huge', ...                 1   \n",
       "1  [{'Word': ['Throughout', 'history', 'from', 't...                 3   \n",
       "2  [{'Word': ['IntroductionIn', 'the', 'span', 'o...                13   \n",
       "3  [{'Word': ['The', 'way', 'we', 'live', 'work',...                 4   \n",
       "4  [{'Word': ['The', 'year', '2040', 'is', 'poise...                 4   \n",
       "5  [{'Word': ['Entertainment', 'is', 'giving', 'p...                 6   \n",
       "6  [{'Word': ['Cybercrime', 'is', 'the', 'most', ...                 1   \n",
       "7  [{'Word': ['Introduction', 'The', 'year', '203...                 3   \n",
       "8  [{'Word': ['As', 'technology', 'continues', 't...                 2   \n",
       "9  [{'Word': ['Understanding', 'Cybercrime', 'An'...                 8   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0                5  \n",
       "1                6  \n",
       "2                6  \n",
       "3                6  \n",
       "4                6  \n",
       "5                6  \n",
       "6                5  \n",
       "7                7  \n",
       "8                6  \n",
       "9                5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
